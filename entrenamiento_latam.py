# -*- coding: utf-8 -*-
"""Entrenamiento_LATAM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HLyFNDXM8T2lBFXTeQJ57vpBXZ7ktQV0
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import datetime as dt
import sklearn
drive.mount('/content/drive')

data = '/content/drive/MyDrive/TFM - Palladium/PROYECTO LIMPIO/latam_sin_covid.csv'
df_data = pd.read_csv(data, sep=",")
df_data

df_data.columns

df_data["MONEDA"].unique()

df_data = pd.get_dummies(df_data, columns = ["MONEDA", "FIDELIDAD","TIPO_CLIENTE"])



#Eliminamos columnas que creemos que no son útiles
df_data.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1', 'LOCALIZADOR', 'ADULTOS', 'NENES', 'BEBES', 'CM_ID_CONTRATO', 'CM_CONTRATO', 'SEGMENTOCREDITO'], axis = 'columns', inplace=True)

#Cambiamos formato de fecha
df_data['FECHA_TTOO'] = pd.to_datetime(df_data['FECHA_TTOO'], format="%Y-%m-%d %H:%M:%S")

#Cambiamos formato de fecha LLEGADA
df_data['LLEGADA'] = pd.to_datetime(df_data['LLEGADA'], format="%Y-%m-%d")

#Creamos columna con los días de pre. de reserva hasta la llegada
df_data["LEAD_TIME"] = (df_data['LLEGADA'] - df_data['FECHA_TTOO']).dt.days

df_data["MES_LLEGADA"] = df_data['LLEGADA'].dt.month

df_data["WEEKDAY_LLEGADA"] = df_data['LLEGADA'].dt.weekday

df_data["WEEK_LLEGADA"] = df_data['LLEGADA'].dt.week

from sklearn.model_selection import KFold

from sklearn.model_selection import StratifiedKFold

folds = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)

from sklearn.model_selection import train_test_split
feature_list=["NOCHES","ADR","PAX", "LEAD_TIME", "FIDELIDAD_Palladium Rewards", "FIDELIDAD_Palladium Connect", "MES_LLEGADA", "WEEKDAY_LLEGADA", "WEEK_LLEGADA", 'MONEDA_Dólar', 'MONEDA_Peso Mexicano', 'COMERCIALIZADORA']
#X = df_data.drop("ESTADO_RESERVA", axis = 1)
X = df_data[feature_list]
feature_names = X.columns
X = X.values

y = df_data["ESTADO_RESERVA"].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=0, test_size=0.2, stratify=y
)

"""Bagging (Random Forest)"""

#Bagging (Random Forest)
#Con none pilla la maoyor profundidad posible, random classfier cuanto menos estimators mas complejo
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

metrics = []

for n, (i_train_fold, i_val_fold) in enumerate(folds.split(X_train, y_train)):

  x = X_train[i_train_fold]
  y = y_train[i_train_fold]
  x_val = X_train[i_val_fold]
  y_val = y_train[i_val_fold]

  rf = RandomForestClassifier(n_estimators = 115, max_depth=	20).fit(x, y)
  y_pred = rf.predict_proba(x)[:, 1]
  y_pred_val = rf.predict_proba(x_val)[:, 1]

  auc_train = roc_auc_score(y, y_pred)
  auc_val = roc_auc_score(y_val, y_pred_val)
  
  metrics.append((auc_train, auc_val))

  print(f"Fold #{n + 1}: auc_train {auc_train: .4f} - auc_val {auc_val: .4f}")

plt.barh(feature_names, rf.feature_importances_)

#creo que no hay que ejecutar
  rf = RandomForestClassifier(n_estimators = 100, max_depth=	20).fit(X_train, y_train)
  y_pred = rf.predict_proba(X_train)[:, 1]
  y_pred_val = rf.predict_proba(X_test)[:, 1]

#creo que no hay que ejecutar
  auc_train = roc_auc_score(y_train, y_pred)
  auc_val = roc_auc_score(y_test, y_pred_val)
  print(auc_train, auc_val)

sklearn.metrics.plot_roc_curve(rf, x_val, y_val)

"""Gradient boosting"""

metrics

#Gradient boosting
#Definir el 75% y el 25% de los datos
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score

for n, (i_train_fold, i_val_fold) in enumerate(folds.split(X_train, y_train)):

    x = X_train[i_train_fold]
    y = y_train[i_train_fold]
    x_val = X_train[i_val_fold]
    y_val = y_train[i_val_fold]

    gb = GradientBoostingClassifier(n_estimators = 300, max_depth=3).fit(x, y)
    y_pred = gb.predict_proba(x)[:, 1]
    y_pred_val = gb.predict_proba(x_val)[:, 1]

    auc_train = roc_auc_score(y, y_pred)
    auc_val = roc_auc_score(y_val, y_pred_val)

    print(f"Fold #{n + 1}: auc_train {auc_train: .4f} - auc_val {auc_val: .4f}")

sklearn.metrics.plot_roc_curve(gb, x_val, y_val)

#LGBMClassifier
#Definir el 75% y el 25% de los datos
#Incrementar valor n_estimators
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score

for n, (i_train_fold, i_val_fold) in enumerate(folds.split(X_train, y_train)):

    x = X_train[i_train_fold]
    y = y_train[i_train_fold]
    x_val = X_train[i_val_fold]
    y_val = y_train[i_val_fold]

    LGBMC = LGBMClassifier(n_estimators = 3000, max_depth=4).fit(x, y)
    y_pred = LGBMC.predict_proba(x)[:, 1]
    y_pred_val = LGBMC.predict_proba(x_val)[:, 1]

    auc_train = roc_auc_score(y, y_pred)
    auc_val = roc_auc_score(y_val, y_pred_val)

    print(f"Fold #{n + 1}: auc_train {auc_train: .4f} - auc_val {auc_val: .4f}")

sklearn.metrics.plot_roc_curve(LGBMC, x_val, y_val)

y_pred_val

#Evaluación de las variables del modelo
#preguntar a gustavo recall o precision?
gb.feature_importances_

plt.barh(feature_names, gb.feature_importances_)